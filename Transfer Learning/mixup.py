# -*- coding: utf-8 -*-
"""Mixup

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RPEJUKvi1EH06ks9CQH1iCge0ryneezd
"""

import pandas as pd
import numpy as np
import random
import torch
import torch.nn as nn
from pytorch_tabnet.utils import define_device

class TabNetMixup:

  def __init__(
    self,
    alpha: float = 2.0,
    use_cuda = True,
    device: str = 'auto',
    random_state: int = 23
    ):

      self.device = device
      self.random_state = random_state
      self.alpha = alpha
      self.use_cuda = use_cuda

  def _set_seed(self):

        torch.manual_seed(self.random_state)
        np.random.seed(self.random_state)
        return

  def __call__(
    self, 
    X, 
    y
  ):
    '''
    Returns: mixed inputs, mixed targets.
    '''
    if self.alpha > 0:
        lam = np.random.beta(self.alpha, self.alpha)
    else:
        lam = 1

    batch_size = X.size()[0]
    if self.use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_X = lam * X + (1 - lam) * X[index, :]
    mixed_y = lam * y + (1 - lam) * y[index, :]

    return mixed_X, mixed_y

def mixup(
  X,
  y, 
  alpha=2, 
  use_cuda=True
  ):
    '''
    Returns: mixed inputs, mixed targets.
    '''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = X.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_X = lam * X + (1 - lam) * X[index, :]
    mixed_y = lam * y + (1 - lam) * y[index, :]

    return mixed_X, mixed_y


class Mixup:
  def __init__(self, x, y, targets):
    '''
    x - original inputs to the model
    y - original targets
    targets - dataframe with train targets
    '''
    self.x = x
    self.y = y
    self.targets = targets
    self.class_counts = targets.sum()
    self.min_index=[]
    
    min_classes = self.targets.sum().loc[lambda a: a<=100].index
    for i in min_classes.tolist():
      self.min_index.append(self.targets.columns.tolist().index(i))

  #============================================================================#

  '''
  Code from original paper
  https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py
  '''
  def simple_mixup(self, alpha=0.5, use_cuda=True):
    '''
    Returns: mixed inputs, pairs of targets, and lambda
    '''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = self.x.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_x = lam * self.x + (1 - lam) * self.x[index, :]
    y_a, y_b = self.y, self.y[index]

    return mixed_x, y_a, y_b, lam

  def mixup_criterion(self,criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

  #============================================================================#

  def remix(self, alpha=1.0, t= 0.5, k=7.5 , num_min_samples = False,use_cuda=True):
    '''Returns remixed inputs and targets'''
    if alpha > 0:
        lamx = np.random.beta(alpha, alpha)
    else:
        lamx = 1

    batch_size = self.x.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)
    
    index = torch.randperm(batch_size)
    mixed_x = lamx * self.x + (1 - lamx) * self.x[index, :]
    
    lamy=None
    mixed_y=[]
    
    if num_min_samples:
      # number of minority classes in each sample
      nj = torch.sum(self.y[:,self.min_index],axis=1)
      ni = torch.sum(self.y[index, :][:,self.min_index],axis=1)

      for i in range(0,len(nj)):
        if (nj[i] >= k*ni[i]) and lamx < t:
          lamy=1
        elif (nj[i] <= 1/k*ni[i]) and (1-lamx < t):
          lamy=0
        else:
          lamy=lamx
      
        mi_y = lamy * self.y[i] + (1 - lamy) * self.y[index, :][i]
        mi_y = mi_y.cpu().numpy()
        mixed_y.append(mi_y)

      if use_cuda:  
        mixed_y=torch.FloatTensor(mixed_y).cuda()
      else:
        mixed_y=torch.FloatTensor(mixed_y)

    else:
      # mean of the number of samples in all target classes, for each sample in the batch
      counts = torch.Tensor(self.class_counts.tolist()).cuda()
      nj= torch.mean((self.y*counts), axis=1)
      ni= torch.mean((self.y[index, :]*counts), axis=1)
      for i in range(0,len(nj)):
        if (nj[i] >= k*ni[i]) and lamx < t:
          lamy=0
        elif (nj[i] <= 1/k*ni[i]) and (1-lamx < t):
          lamy=1
        else:
          lamy=lamx
      
        mi_y = lamy * self.y[i] + (1 - lamy) * self.y[index, :][i]
        mi_y = mi_y.cpu().numpy()
        mixed_y.append(mi_y)

      if use_cuda:  
        mixed_y=torch.FloatTensor(mixed_y).cuda()
      else:
        mixed_y=torch.FloatTensor(mixed_y)

    return mixed_x, mixed_y